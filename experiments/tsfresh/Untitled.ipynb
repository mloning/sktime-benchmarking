{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import os \n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import wilcoxon as _wilcoxon \n",
    "from scipy.stats import binom_test as _binom_test\n",
    "from itertools import combinations\n",
    "from IPython.display import display\n",
    "from itertools import product\n",
    "import seaborn as sns\n",
    "from sktime.benchmarking.evaluation import Evaluator\n",
    "from sktime.benchmarking.results import HDDResults\n",
    "from sktime.benchmarking.metrics import PairwiseMetric\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sktime.series_as_features.model_selection import PresplitFilesCV\n",
    "from joblib import load\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wilcoxon_test(x, y):\n",
    "    return _wilcoxon(x, y).pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binom_test(x, y):\n",
    "    n = len(x)\n",
    "    x_wins = np.sum(x > y)\n",
    "\n",
    "    # draws support the null-hypothesis, so we do not discount them but split them \n",
    "    # evenly between the two estimators; if there is an odd number of them, we ignore one.\n",
    "    draws = np.sum(x == y)\n",
    "    if draws > 0:\n",
    "        if draws % 2 != 0:\n",
    "            n -= 1\n",
    "        x_wins += draws // 2\n",
    "\n",
    "    return _binom_test(x_wins, n=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.560862202981874"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wilcoxon_test(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44767342529727316"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binom_test(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ucfamml/.conda/envs/sktime/bin/python\n"
     ]
    }
   ],
   "source": [
    "def compare_results(x, y):\n",
    "    a = x.values\n",
    "    b = y.values\n",
    "\n",
    "    # wilcoxon test\n",
    "    pwil = wilcoxon(a, b).pvalue\n",
    "    \n",
    "    # binomial test\n",
    "    x_wins = np.mean(a > b)\n",
    "    y_wins = np.mean(b > a)\n",
    "    draw = np.mean(a == b)\n",
    "    pbin = binom_test(np.sum(x_wins), n=x.shape[0], p=0.5, alternative='two-sided')\n",
    "    diff = x - y\n",
    "    \n",
    "    # combine results\n",
    "    results = pd.Series({'wilcoxon_pval': pwil,\n",
    "                        'x_wins': x_wins, \n",
    "                        'y_wins': y_wins, \n",
    "                        'draw': draw,\n",
    "                        'binomial_pval': pbin})\n",
    "    results = pd.concat([results, diff.describe()], axis=0)\n",
    "    # display results\n",
    "    display(pd.DataFrame(results).T.drop(columns='count').round(3))\n",
    "    \n",
    "    # scatter plot\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.scatter(a, b)\n",
    "    ax.plot([0, np.max([np.max(x), np.max(y)])], \n",
    "            [0, np.max([np.max(x), np.max(y)])], \n",
    "            'red', linewidth=1)\n",
    "    # ax.set_aspect('equal')\n",
    "    ax.set(xlabel=x.name, ylabel=y.name);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME = os.path.expanduser(\"~\")\n",
    "REPO = os.path.join(HOME, \"Documents/Research/software/sktime/sktime-benchmarking/\")\n",
    "EXPERIMENT = os.path.join(REPO, \"experiments/tsfresh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiments/tsfresh/results/results.pickle\n",
    "results = load(os.path.join(EXPERIMENT, \"results\", \"results.pickle\"))\n",
    "results.cv = PresplitFilesCV()\n",
    "\n",
    "evaluator = Evaluator(results=results)\n",
    "metric = PairwiseMetric(func=accuracy_score, name=\"accuracy\")\n",
    "evaluator.evaluate(metric)\n",
    "evaluator.metrics_by_strategy_dataset.to_csv(\"accuracy.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsfresh = pd.read_csv(\"accuracy.csv\", index_col=0, header=0)\n",
    "tsfresh = tsfresh.drop(columns=\"accuracy_stderr\").pivot(index=\"dataset\", columns=\"strategy\", values=\"accuracy_mean\")\n",
    "\n",
    "results = pd.read_csv(os.path.join(REPO, \"results\", \"Resamples.csv\"), index_col=0)  # published results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HIVE-COTE                    2.83750\n",
       "Flat-COTE                    4.65625\n",
       "ST                           8.98125\n",
       "BOSS                         9.58125\n",
       "EE                          11.16875\n",
       "DTW_F                       13.72500\n",
       "TSF                         14.64375\n",
       "TSBF                        14.85000\n",
       "LS                          15.48125\n",
       "tsfresh-rf-efficient-200    15.60625\n",
       "RotF                        16.79375\n",
       "MSM_1NN                     16.88750\n",
       "LPS                         16.96250\n",
       "DD_DTW                      18.76250\n",
       "DTD_C                       19.10000\n",
       "LCSS_1NN                    19.20000\n",
       "TWE_1NN                     19.25625\n",
       "CID_DTW                     19.32500\n",
       "WDTW_1NN                    19.62500\n",
       "DTW_Rn_1NN                  20.43125\n",
       "ERP_1NN                     20.46250\n",
       "RandF                       20.68125\n",
       "WDDTW_1NN                   21.62500\n",
       "SVMQ                        22.16250\n",
       "DDTW_Rn_1NN                 22.37500\n",
       "PS                          22.41875\n",
       "DTW_R1_1NN                  23.18750\n",
       "MLP                         23.37500\n",
       "SAXVSM                      23.50625\n",
       "ACF                         23.66875\n",
       "DDTW_R1_1NN                 24.96250\n",
       "BoP                         25.07500\n",
       "SVML                        25.63125\n",
       "Euclidean_1NN               26.83750\n",
       "FS                          28.20625\n",
       "BN                          29.47500\n",
       "NB                          31.56875\n",
       "Logistic                    32.94375\n",
       "C45                         33.96250\n",
       "dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = pd.merge(results, tsfresh, left_index=True, right_index=True).T\n",
    "\n",
    "r.rank(ascending=False).mean(axis=1).sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
